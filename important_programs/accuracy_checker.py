#!/usr/bin/env python3
"""
æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã®ç²¾åº¦æ¤œè¨¼ãƒ„ãƒ¼ãƒ«
results.txtã®å®Ÿéš›ã®å€¤ã¨æ¯”è¼ƒã—ã¦ç²¾åº¦ã‚’è©•ä¾¡
"""

import os
import csv
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
from scipy.signal import find_peaks

class AccuracyChecker:
    """ç²¾åº¦æ¤œè¨¼ã‚¯ãƒ©ã‚¹"""
    
    # å®šæ•°å®šç¾©
    BASE_BALL_COUNT = 30000  # ç²¾åº¦è¨ˆç®—ã®åŸºæº–å€¤
    HIGH_ACCURACY_THRESHOLD = 80  # é«˜ç²¾åº¦ã®é–¾å€¤
    MAX_DIFF = 20000  # é€£ç¶šã™ã‚‹å€¤ã®æœ€å¤§å·®åˆ†ï¼ˆå¤§å½“ãŸã‚Šæ™‚ã®æ€¥æ¿€ãªå¤‰åŒ–ã‚’è€ƒæ…®ï¼‰
    
    def __init__(self, results_file="results.txt", extracted_data_dir="graphs/unified_extracted_data"):
        self.results_file = results_file
        self.extracted_data_dir = extracted_data_dir
        self.results_data = self.load_results()
        
    def load_results(self) -> Dict[str, Dict]:
        """results.txtã‚’èª­ã¿è¾¼ã¿"""
        if not os.path.exists(self.results_file):
            raise FileNotFoundError(f"Results file not found: {self.results_file}")
        
        data = {}
        with open(self.results_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ã‚­ãƒ¼ã«ã—ã¦ä¿å­˜
                base_name = row['ç”»åƒå'].replace('.jpg', '')
                data[base_name] = {
                    'å°ç•ªå·': row['å°ç•ªå·'],
                    'æ©Ÿç¨®': row['æ©Ÿç¨®'],
                    'å®Ÿéš›ã®æœ€å¤§å€¤': self.parse_number(row['å®Ÿéš›ã®æœ€å¤§å€¤']),
                    'å®Ÿéš›ã®æœ€çµ‚å·®ç‰': self.parse_number(row['å®Ÿéš›ã®æœ€çµ‚å·®ç‰']),
                    'ç›®è¦–äºˆæƒ³æœ€çµ‚å·®ç‰': self.parse_number(row['ç›®è¦–äºˆæƒ³æœ€çµ‚å·®ç‰']),
                    'èª¤å·®': self.parse_number(row['èª¤å·®'])
                }
        return data
    
    def parse_number(self, value: str) -> Optional[float]:
        """ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®æ•°å€¤ã‚’ãƒ‘ãƒ¼ã‚¹"""
        if not value or value == '':
            return None
        # ã‚«ãƒ³ãƒã¨å¼•ç”¨ç¬¦ã‚’é™¤å»
        cleaned = value.strip('"').replace(',', '')
        # ãƒã‚¤ãƒŠã‚¹è¨˜å·ã®å‡¦ç†
        if cleaned.startswith('-'):
            return -float(cleaned[1:])
        return float(cleaned)
    
    def validate_extracted_data(self, df: pd.DataFrame) -> List[int]:
        """æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ã‚’æ¤œè¨¼ã—ã€ç•°å¸¸ãªç®‡æ‰€ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¿”ã™"""
        anomalies = []
        
        for i in range(1, len(df)):
            diff = abs(df.iloc[i]['value'] - df.iloc[i-1]['value'])
            if diff > self.MAX_DIFF:
                anomalies.append(i)
        
        return anomalies
    
    def find_peaks_and_valleys(self, df: pd.DataFrame) -> Dict[str, List[Dict]]:
        """ã‚°ãƒ©ãƒ•ã®é ‚ç‚¹ï¼ˆãƒ”ãƒ¼ã‚¯ã¨è°·ï¼‰ã‚’æ¤œå‡º"""
        values = df['value'].values
        
        # ãƒ”ãƒ¼ã‚¯ï¼ˆæ¥µå¤§å€¤ï¼‰æ¤œå‡º
        peaks, peak_props = find_peaks(values, 
                                       prominence=1000,  # æœ€å°çªå‡ºåº¦
                                       distance=10)      # æœ€å°é–“éš”
        
        # è°·ï¼ˆæ¥µå°å€¤ï¼‰æ¤œå‡º
        valleys, valley_props = find_peaks(-values, 
                                           prominence=1000,
                                           distance=10)
        
        return {
            'peaks': [{'index': int(i), 'value': float(values[i])} for i in peaks],
            'valleys': [{'index': int(i), 'value': float(values[i])} for i in valleys]
        }
    
    def verify_true_maximum(self, df: pd.DataFrame, reported_max: float) -> Dict:
        """å ±å‘Šã•ã‚ŒãŸæœ€å¤§å€¤ãŒçœŸã®é ‚ç‚¹ã‹ã‚’æ¤œè¨¼"""
        peaks_valleys = self.find_peaks_and_valleys(df)
        all_peaks = [p['value'] for p in peaks_valleys['peaks']]
        
        if not all_peaks:
            return {'is_peak': False, 'confidence': 0, 'detected_max': reported_max, 'peak_count': 0}
        
        actual_max = max(all_peaks)
        max_index = df['value'].idxmax()
        
        # æœ€å¤§å€¤ãŒæ¤œå‡ºã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯ã®ä¸€ã¤ã‹ãƒã‚§ãƒƒã‚¯
        is_detected_peak = any(p['index'] == max_index for p in peaks_valleys['peaks'])
        
        # ä¿¡é ¼åº¦è¨ˆç®—
        if is_detected_peak and abs(actual_max - reported_max) < 100:
            confidence = 100
        elif abs(actual_max - reported_max) < 500:
            confidence = 80
        else:
            confidence = 50
        
        return {
            'is_peak': is_detected_peak,
            'confidence': confidence,
            'detected_max': actual_max,
            'peak_count': len(all_peaks)
        }
    
    def load_extracted_data(self, image_name: str) -> Tuple[Optional[float], Optional[float], int, Dict]:
        """æŠ½å‡ºã•ã‚ŒãŸCSVãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æœ€å¤§å€¤ã€æœ€çµ‚å€¤ã€ç•°å¸¸å€¤æ•°ã€é ‚ç‚¹æƒ…å ±ã‚’å–å¾—"""
        csv_path = os.path.join(self.extracted_data_dir, f"{image_name}_optimal_data.csv")
        
        if not os.path.exists(csv_path):
            return None, None, 0, {}
        
        df = pd.read_csv(csv_path)
        if df.empty:
            return None, None, 0, {}
        
        if 'value' not in df.columns:
            raise ValueError(f"'value' column not found in {csv_path}")
        
        max_value = df['value'].max()
        final_value = df['value'].iloc[-1]
        
        # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
        anomalies = self.validate_extracted_data(df)
        anomaly_count = len(anomalies)
        
        # é ‚ç‚¹æ¤œè¨¼
        peak_info = self.verify_true_maximum(df, max_value)
        
        return max_value, final_value, anomaly_count, peak_info
    
    def calculate_accuracy(self, actual: float, predicted: float, peak_info: Dict = None) -> Dict:
        """ç²¾åº¦æŒ‡æ¨™ã‚’è¨ˆç®—ï¼ˆé ‚ç‚¹æƒ…å ±ã‚’è€ƒæ…®ï¼‰"""
        if actual is None or predicted is None:
            return {
                'error': None,
                'absolute_error': None,
                'percentage_error': None,
                'accuracy': None,
                'adjusted_accuracy': None,
                'peak_confidence': None
            }
        
        error = predicted - actual
        absolute_error = abs(error)
        
        # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ã‚¨ãƒ©ãƒ¼ï¼ˆå®Ÿéš›ã®å€¤ãŒ0ã®å ´åˆã®å‡¦ç†ï¼‰
        if actual != 0:
            percentage_error = (absolute_error / abs(actual)) * 100
        else:
            percentage_error = 100 if predicted != 0 else 0
        
        # ç²¾åº¦ã‚¹ã‚³ã‚¢ï¼ˆ0-100%ï¼‰
        # BASE_BALL_COUNTç‰ã‚’åŸºæº–ã«ã—ã¦ç²¾åº¦ã‚’è¨ˆç®—
        accuracy = max(0, 100 - (absolute_error / self.BASE_BALL_COUNT * 100))
        
        result = {
            'error': error,
            'absolute_error': absolute_error,
            'percentage_error': percentage_error,
            'accuracy': accuracy,
            'adjusted_accuracy': accuracy,
            'peak_confidence': None
        }
        
        # é ‚ç‚¹æƒ…å ±ã‚’è€ƒæ…®ã—ãŸèª¿æ•´ç²¾åº¦
        if peak_info and peak_info.get('confidence'):
            adjusted_accuracy = accuracy * (peak_info['confidence'] / 100)
            result['adjusted_accuracy'] = adjusted_accuracy
            result['peak_confidence'] = peak_info['confidence']
        
        return result
    
    def analyze_all(self) -> pd.DataFrame:
        """å…¨ç”»åƒã®ç²¾åº¦ã‚’åˆ†æ"""
        results = []
        
        for image_name, actual_data in self.results_data.items():
            # æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            extracted_max, extracted_final, anomaly_count, peak_info = self.load_extracted_data(image_name)
            
            # æœ€çµ‚å€¤ã®ç²¾åº¦ã‚’è¨ˆç®—
            final_accuracy = self.calculate_accuracy(
                actual_data['å®Ÿéš›ã®æœ€çµ‚å·®ç‰'],
                extracted_final
            )
            
            # æœ€å¤§å€¤ã®ç²¾åº¦ã‚’è¨ˆç®—ï¼ˆé ‚ç‚¹æƒ…å ±ã‚’å«ã‚€ï¼‰
            max_accuracy = self.calculate_accuracy(
                actual_data['å®Ÿéš›ã®æœ€å¤§å€¤'],
                extracted_max,
                peak_info
            )
            
            results.append({
                'ç”»åƒå': image_name,
                'å°ç•ªå·': actual_data['å°ç•ªå·'],
                'æ©Ÿç¨®': actual_data['æ©Ÿç¨®'],
                # å®Ÿéš›ã®å€¤
                'å®Ÿéš›ã®æœ€å¤§å€¤': actual_data['å®Ÿéš›ã®æœ€å¤§å€¤'],
                'å®Ÿéš›ã®æœ€çµ‚å€¤': actual_data['å®Ÿéš›ã®æœ€çµ‚å·®ç‰'],
                # æŠ½å‡ºå€¤
                'æŠ½å‡ºæœ€å¤§å€¤': extracted_max,
                'æŠ½å‡ºæœ€çµ‚å€¤': extracted_final,
                # æœ€çµ‚å€¤ã®ç²¾åº¦
                'æœ€çµ‚å€¤èª¤å·®': final_accuracy['error'],
                'æœ€çµ‚å€¤çµ¶å¯¾èª¤å·®': final_accuracy['absolute_error'],
                'æœ€çµ‚å€¤èª¤å·®ç‡(%)': final_accuracy['percentage_error'],
                'æœ€çµ‚å€¤ç²¾åº¦(%)': final_accuracy['accuracy'],
                # æœ€å¤§å€¤ã®ç²¾åº¦
                'æœ€å¤§å€¤èª¤å·®': max_accuracy['error'],
                'æœ€å¤§å€¤çµ¶å¯¾èª¤å·®': max_accuracy['absolute_error'],
                'æœ€å¤§å€¤èª¤å·®ç‡(%)': max_accuracy['percentage_error'],
                'æœ€å¤§å€¤ç²¾åº¦(%)': max_accuracy['accuracy'],
                # é ‚ç‚¹æ¤œå‡ºæƒ…å ±
                'é ‚ç‚¹æ¤œå‡ºä¿¡é ¼åº¦': peak_info.get('confidence', 0) if peak_info else 0,
                'ãƒ”ãƒ¼ã‚¯æ•°': peak_info.get('peak_count', 0) if peak_info else 0,
                # ãƒ‡ãƒ¼ã‚¿å“è³ª
                'ç•°å¸¸å€¤æ•°': anomaly_count,
                'ãƒ‡ãƒ¼ã‚¿å“è³ª': 'OK' if anomaly_count == 0 else 'NG',
                # ç›®è¦–äºˆæƒ³ã¨ã®æ¯”è¼ƒ
                'ç›®è¦–äºˆæƒ³': actual_data['ç›®è¦–äºˆæƒ³æœ€çµ‚å·®ç‰'],
                'ç›®è¦–èª¤å·®': actual_data['èª¤å·®']
            })
        
        return pd.DataFrame(results)
    
    def print_summary(self, df: pd.DataFrame):
        """ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        print("\n" + "="*80)
        print("ğŸ¯ ç²¾åº¦æ¤œè¨¼çµæœã‚µãƒãƒªãƒ¼")
        print("="*80)
        
        # æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã®ã¿ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        valid_final = df[df['æœ€çµ‚å€¤ç²¾åº¦(%)'].notna()]
        valid_max = df[df['æœ€å¤§å€¤ç²¾åº¦(%)'].notna()]
        
        if not valid_final.empty:
            print("\nğŸ“Š æœ€çµ‚å€¤ã®ç²¾åº¦:")
            print(f"  å¹³å‡ç²¾åº¦: {valid_final['æœ€çµ‚å€¤ç²¾åº¦(%)'].mean():.1f}%")
            print(f"  æœ€é«˜ç²¾åº¦: {valid_final['æœ€çµ‚å€¤ç²¾åº¦(%)'].max():.1f}%")
            print(f"  æœ€ä½ç²¾åº¦: {valid_final['æœ€çµ‚å€¤ç²¾åº¦(%)'].min():.1f}%")
            print(f"  å¹³å‡çµ¶å¯¾èª¤å·®: {valid_final['æœ€çµ‚å€¤çµ¶å¯¾èª¤å·®'].mean():.0f}ç‰")
            
            # é«˜ç²¾åº¦ï¼ˆHIGH_ACCURACY_THRESHOLD%ä»¥ä¸Šï¼‰ã®ã‚±ãƒ¼ã‚¹
            high_accuracy = valid_final[valid_final['æœ€çµ‚å€¤ç²¾åº¦(%)'] >= self.HIGH_ACCURACY_THRESHOLD]
            print(f"  é«˜ç²¾åº¦({self.HIGH_ACCURACY_THRESHOLD}%ä»¥ä¸Š): {len(high_accuracy)}/{len(valid_final)}ä»¶")
            
            # ãƒ‡ãƒ¼ã‚¿å“è³ªæƒ…å ±
            good_quality = valid_final[valid_final['ãƒ‡ãƒ¼ã‚¿å“è³ª'] == 'OK']
            print(f"  ãƒ‡ãƒ¼ã‚¿å“è³ªOK: {len(good_quality)}/{len(valid_final)}ä»¶")
        
        if not valid_max.empty:
            print("\nğŸ“Š æœ€å¤§å€¤ã®ç²¾åº¦:")
            print(f"  å¹³å‡ç²¾åº¦: {valid_max['æœ€å¤§å€¤ç²¾åº¦(%)'].mean():.1f}%")
            print(f"  æœ€é«˜ç²¾åº¦: {valid_max['æœ€å¤§å€¤ç²¾åº¦(%)'].max():.1f}%")
            print(f"  æœ€ä½ç²¾åº¦: {valid_max['æœ€å¤§å€¤ç²¾åº¦(%)'].min():.1f}%")
            print(f"  å¹³å‡çµ¶å¯¾èª¤å·®: {valid_max['æœ€å¤§å€¤çµ¶å¯¾èª¤å·®'].mean():.0f}ç‰")
        
        print("\nğŸ“ˆ æ©Ÿç¨®åˆ¥ã®ç²¾åº¦:")
        for machine_type in df['æ©Ÿç¨®'].unique():
            machine_data = valid_final[valid_final['æ©Ÿç¨®'] == machine_type]
            if not machine_data.empty:
                print(f"  {machine_type}: å¹³å‡ç²¾åº¦ {machine_data['æœ€çµ‚å€¤ç²¾åº¦(%)'].mean():.1f}%")
        
        # æœ€ã‚‚ç²¾åº¦ãŒé«˜ã‹ã£ãŸç”»åƒ
        if not valid_final.empty:
            best = valid_final.loc[valid_final['æœ€çµ‚å€¤ç²¾åº¦(%)'].idxmax()]
            print(f"\nâœ… æœ€é«˜ç²¾åº¦: {best['ç”»åƒå']} (ç²¾åº¦: {best['æœ€çµ‚å€¤ç²¾åº¦(%)']:.1f}%)")
            print(f"   å®Ÿéš›: {best['å®Ÿéš›ã®æœ€çµ‚å€¤']:.0f}, æŠ½å‡º: {best['æŠ½å‡ºæœ€çµ‚å€¤']:.0f}, èª¤å·®: {best['æœ€çµ‚å€¤çµ¶å¯¾èª¤å·®']:.0f}")
        
        # æœ€ã‚‚ç²¾åº¦ãŒä½ã‹ã£ãŸç”»åƒ
        if not valid_final.empty:
            worst = valid_final.loc[valid_final['æœ€çµ‚å€¤ç²¾åº¦(%)'].idxmin()]
            print(f"\nâŒ æœ€ä½ç²¾åº¦: {worst['ç”»åƒå']} (ç²¾åº¦: {worst['æœ€çµ‚å€¤ç²¾åº¦(%)']:.1f}%)")
            print(f"   å®Ÿéš›: {worst['å®Ÿéš›ã®æœ€çµ‚å€¤']:.0f}, æŠ½å‡º: {worst['æŠ½å‡ºæœ€çµ‚å€¤']:.0f}, èª¤å·®: {worst['æœ€çµ‚å€¤çµ¶å¯¾èª¤å·®']:.0f}")
        
        # é ‚ç‚¹æ¤œå‡ºç²¾åº¦
        print("\nğŸ¯ é ‚ç‚¹æ¤œå‡ºç²¾åº¦:")
        peak_detected = df[df['é ‚ç‚¹æ¤œå‡ºä¿¡é ¼åº¦'] > 0]
        if not peak_detected.empty:
            high_confidence = peak_detected[peak_detected['é ‚ç‚¹æ¤œå‡ºä¿¡é ¼åº¦'] >= 80]
            print(f"  é«˜ä¿¡é ¼åº¦(80%ä»¥ä¸Š): {len(high_confidence)}/{len(peak_detected)}ä»¶")
            print(f"  å¹³å‡ä¿¡é ¼åº¦: {peak_detected['é ‚ç‚¹æ¤œå‡ºä¿¡é ¼åº¦'].mean():.1f}%")
            print(f"  å¹³å‡ãƒ”ãƒ¼ã‚¯æ•°: {peak_detected['ãƒ”ãƒ¼ã‚¯æ•°'].mean():.1f}å€‹")
    
    def save_report(self, df: pd.DataFrame, output_file: str = "accuracy_report.csv"):
        """è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜"""
        # æ•°å€¤ã‚’è¦‹ã‚„ã™ããƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        for col in df.columns:
            if 'ç²¾åº¦(%)' in col or 'èª¤å·®ç‡(%)' in col:
                df[col] = df[col].round(1)
            elif 'å€¤' in col or 'èª¤å·®' in col:
                if col not in ['ç”»åƒå', 'å°ç•ªå·', 'æ©Ÿç¨®']:
                    df[col] = df[col].round(0)
        
        df.to_csv(output_file, index=False, encoding='utf-8')
        print(f"\nğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {output_file}")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸ” æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ç²¾åº¦æ¤œè¨¼ãƒ„ãƒ¼ãƒ«")
    print("ğŸ“Š results.txtã¨æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã‚’æ¯”è¼ƒã—ã¾ã™")
    
    # ç²¾åº¦ãƒã‚§ãƒƒã‚«ãƒ¼ã‚’åˆæœŸåŒ–
    checker = AccuracyChecker()
    
    # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æ
    results_df = checker.analyze_all()
    
    # ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
    checker.print_summary(results_df)
    
    # è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜
    checker.save_report(results_df)
    
    # å€‹åˆ¥ã®çµæœã‚’è¡¨ç¤ºï¼ˆä¸Šä½5ä»¶ï¼‰
    print("\n" + "="*80)
    print("ğŸ“‹ å€‹åˆ¥çµæœï¼ˆç²¾åº¦ä¸Šä½5ä»¶ï¼‰:")
    print("="*80)
    
    valid_df = results_df[results_df['æœ€çµ‚å€¤ç²¾åº¦(%)'].notna()].sort_values('æœ€çµ‚å€¤ç²¾åº¦(%)', ascending=False)
    
    for idx, row in valid_df.head(5).iterrows():
        print(f"\n{row['ç”»åƒå']}:")
        print(f"  å°ç•ªå·: {row['å°ç•ªå·']}, æ©Ÿç¨®: {row['æ©Ÿç¨®']}")
        print(f"  å®Ÿéš›ã®æœ€çµ‚å€¤: {row['å®Ÿéš›ã®æœ€çµ‚å€¤']:.0f}")
        print(f"  æŠ½å‡ºæœ€çµ‚å€¤: {row['æŠ½å‡ºæœ€çµ‚å€¤']:.0f}")
        print(f"  èª¤å·®: {row['æœ€çµ‚å€¤èª¤å·®']:.0f} (ç²¾åº¦: {row['æœ€çµ‚å€¤ç²¾åº¦(%)']:.1f}%)")

if __name__ == "__main__":
    main()